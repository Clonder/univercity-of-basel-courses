{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3500e666",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train and test set sizes: 36176 9044\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "def laplace_mech(v, sensitivity, epsilon):\n",
    "    return v + np.random.laplace(loc=0, scale=sensitivity / epsilon)\n",
    "\n",
    "def gaussian_mech(v, sensitivity, epsilon, delta):\n",
    "    return v + np.random.normal(loc=0, scale=sensitivity * np.sqrt(2*np.log(1.25/delta)) / epsilon)\n",
    "\n",
    "def gaussian_mech_vec(vec, sensitivity, epsilon, delta):\n",
    "    return [v + np.random.normal(loc=0, scale=sensitivity * np.sqrt(2*np.log(1.25/delta)) / epsilon)\n",
    "            for v in vec]\n",
    "\n",
    "def gaussian_mech_RDP_vec(vec, sensitivity, alpha, epsilon):\n",
    "    sigma = np.sqrt((sensitivity**2 * alpha) / (2 * epsilon))\n",
    "    return [v + np.random.normal(loc=0, scale=sigma) for v in vec]\n",
    "\n",
    "\n",
    "def loss(weights, xi, yi):\n",
    "    exponent = - yi * (xi.dot(weights))\n",
    "    return np.log(1 + np.exp(exponent))\n",
    "\n",
    "\n",
    "def gradient(weights, xi, yi):\n",
    "    exponent = yi * (xi.dot(weights))\n",
    "    return - (yi*xi) / (1+np.exp(exponent))\n",
    "\n",
    "def predict(xi, weights, bias=0):\n",
    "    label = np.sign(xi @ weights + bias)\n",
    "    return label\n",
    "\n",
    "def accuracy(weights):\n",
    "    return np.sum(predict(X_test, weights) == y_test)/X_test.shape[0]\n",
    "\n",
    "\n",
    "X = np.load('adult_processed_x.npy')\n",
    "y = np.load('adult_processed_y.npy')\n",
    "\n",
    "training_size = int(X.shape[0] * 0.8)\n",
    "\n",
    "X_train = X[:training_size]\n",
    "X_test = X[training_size:]\n",
    "\n",
    "y_train = y[:training_size]\n",
    "y_test = y[training_size:]\n",
    "\n",
    "print('Train and test set sizes:', len(y_train), len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "81e1f90e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model coefficients: [ 7.25273625e-01  5.28692744e-02  2.16164416e-01  3.15914290e-01\n",
      " -3.36810399e-01 -1.17398041e-01 -7.92758753e-01 -4.76246610e-01\n",
      " -4.75825890e-01 -3.48796888e-01 -4.75751160e-01 -1.04259054e-01\n",
      " -5.00962087e-01 -6.00407682e-01  9.53302018e-02  1.75102572e-01\n",
      "  5.11977080e-01  9.32111483e-01 -6.71637360e-03  7.18722508e-01\n",
      " -7.89427608e-01  1.21778497e+00  1.90618957e-01 -6.43742600e-01\n",
      "  1.72193131e+00  1.53039377e+00 -3.43725509e-01 -1.05523627e+00\n",
      " -6.42126608e-01 -5.04239687e-01  8.76083921e-02  1.26578768e-01\n",
      "  1.43387358e-01  8.63652165e-01 -7.57995142e-01 -5.51821340e-01\n",
      " -1.80600184e-01 -7.92412364e-01 -1.12385567e+00  6.14533505e-01\n",
      "  6.16375499e-01  3.62331762e-01  6.48779468e-01  6.69219649e-03\n",
      " -9.51169529e-02  3.05253385e-01 -4.86064983e-01 -7.56624074e-01\n",
      "  1.07014947e-01  9.88792092e-01 -1.80816970e-01  3.33349291e-01\n",
      " -1.41148528e-01 -6.66401519e-02  1.18510773e-01 -3.02875788e-01\n",
      "  3.66130202e-01  9.42949358e-01  6.69469573e-01 -2.38027432e-01\n",
      " -1.26519752e+00  2.30996845e-01 -5.49404983e-01 -2.39422314e-01\n",
      " -2.02536882e-01  7.36024394e-01  9.45812205e-01  8.11349601e-02\n",
      "  1.53959784e-01  5.42489540e-02  3.86362759e-01 -1.97809489e-02\n",
      "  9.82569309e-02 -1.96867627e-01  5.47681160e-01  3.26566756e-02\n",
      "  3.34939883e-01  7.55120932e-01  8.69769360e-01  3.48657375e-01\n",
      " -3.19476797e-02 -6.24608534e-01 -4.37040156e-01 -3.37529082e-01\n",
      " -2.97926410e-01 -5.66938594e-01  3.18577288e-01  6.61505957e-02\n",
      "  3.89032237e-01 -1.13521217e-01 -8.42570890e-01 -7.23171881e-01\n",
      " -9.80657692e-02 -5.40958153e-01 -4.25149621e-01  3.68654924e-01\n",
      " -9.46138922e-01  4.29602842e-01  2.28572664e+00  9.85528480e-01\n",
      "  2.42349865e+00  1.86925581e+01  2.61804321e+00  2.75982521e+00]\n",
      "Model accuracy: 0.8448695267580717\n"
     ]
    }
   ],
   "source": [
    "def train_model():\n",
    "    # Initialize logistic regression model\n",
    "    model = LogisticRegression(max_iter=1000)\n",
    "\n",
    "    # Train the model\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    return model\n",
    "\n",
    "model = train_model()\n",
    "print('Model coefficients:', model.coef_[0])\n",
    "print('Model accuracy:', np.sum(model.predict(X_test) == y_test)/X_test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "13dc8f8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final accuracy: 0.7784166298098186\n"
     ]
    }
   ],
   "source": [
    "def L2_clip(v: np.ndarray, C: float) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Clip a vector by its L2 norm.\n",
    "    \"\"\"\n",
    "    norm = np.linalg.norm(v, ord=2)\n",
    "    factor = min(1, C / norm)\n",
    "    return v * factor\n",
    "\n",
    "def gradient_sum(weights: np.ndarray, X: np.ndarray, y: np.ndarray, C: float) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Compute the sum of L2-clipped gradients for the current weights.\n",
    "    \n",
    "    Parameters:\n",
    "    weights (np.ndarray): The current weights of the model.\n",
    "    X (np.ndarray): The training data features.\n",
    "    y (np.ndarray): The training data labels.\n",
    "    C (float): The clipping parameter.\n",
    "    \"\"\"\n",
    "    gradients = np.array([L2_clip(gradient(weights, x_i, y_i), C) for x_i, y_i in zip(X, y)])\n",
    "    return np.sum(gradients, axis=0)\n",
    "\n",
    "def noisy_gradient_descent_RDP(iterations: int, alpha: float, epsilon_bar: float) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Perform noisy gradient descent with Renyi Differential Privacy.\n",
    "    \n",
    "    Parameters:\n",
    "    iterations (int): Number of iterations for gradient descent.\n",
    "    alpha (float): The order of Renyi Divergence.\n",
    "    epsilon_bar (float): Total privacy budget.\n",
    "    \"\"\"\n",
    "    epsilon_iteration = (0.9 * epsilon_bar) / iterations\n",
    "    weights = np.zeros(X_train.shape[1])\n",
    "    C = 4\n",
    "\n",
    "    noisy_count = laplace_mech(X_train.shape[0], 1, 0.1 * epsilon_bar)\n",
    "\n",
    "    for i in range(iterations):\n",
    "        clipped_gradient_sum = gradient_sum(weights, X_train, y_train, C)\n",
    "        noisy_gradient_sum = gaussian_mech_RDP_vec(clipped_gradient_sum, C, alpha, epsilon_iteration)\n",
    "        noisy_gradient_sum = np.array(noisy_gradient_sum)  # Convert list to numpy array\n",
    "        noisy_avg_gradient = noisy_gradient_sum / noisy_count\n",
    "        weights -= noisy_avg_gradient\n",
    "\n",
    "    return weights\n",
    "\n",
    "weights = noisy_gradient_descent_RDP(10, 20, 0.1)\n",
    "print('Final accuracy:', accuracy(weights))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "691c992d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "- The total privacy cost of this solution is $(\\alpha, \\bar{\\epsilon})$. This is achieved by allocating 90% of $\\bar{\\epsilon}$ across the iterations and using 10% of $\\bar{\\epsilon}$ for the noisy count. Specifically, $\\alpha$ is applied per iteration as defined by the Renyi Differential Privacy framework.\n",
    "- The noisy gradient descent method with Renyi Differential Privacy (RDP) tends to be less accurate than a standard Logistic Regression model because:\n",
    "\n",
    "1. Noise Addition:\n",
    "   - To ensure privacy, the noisy gradient descent method adds noise to the gradients during the training process.\n",
    "2. Clipping:\n",
    "   - The gradients are clipped to ensure they do not exceed a certain norm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e92fd90c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final accuracy: 0.7516585581601062\n"
     ]
    }
   ],
   "source": [
    "def dpsgd(iterations, epsilon, delta, learning_rate, batch_size, C):\n",
    "    \"\"\"\n",
    "    Perform Differentially Private Stochastic Gradient Descent (DPSGD).\n",
    "\n",
    "    Parameters:\n",
    "    iterations (int): Number of iterations for gradient descent.\n",
    "    epsilon (float): Privacy budget parameter.\n",
    "    delta (float): Privacy parameter.\n",
    "    learning_rate (float): Learning rate for the gradient descent.\n",
    "    batch_size (int): Size of the mini-batch for each iteration.\n",
    "    C (float): Clipping parameter for the gradient.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize weights to zeros\n",
    "    weights = np.zeros(X_train.shape[1])\n",
    "    \n",
    "    # Number of samples in the training data\n",
    "    num_samples = X_train.shape[0]\n",
    "    \n",
    "    # Compute the noisy count using Laplace mechanism\n",
    "    noisy_count = laplace_mech(batch_size, 1, epsilon)\n",
    "\n",
    "    for i in range(iterations):\n",
    "        # Randomly sample a mini-batch from the training data\n",
    "        batch_indices = np.random.choice(num_samples, size=batch_size, replace=False)\n",
    "        X_batch = X_train[batch_indices]\n",
    "        y_batch = y_train[batch_indices]\n",
    "        \n",
    "        # Compute the clipped gradient sum for the mini-batch\n",
    "        clipped_gradient_sum = gradient_sum(weights, X_batch, y_batch, C)\n",
    "        \n",
    "        # Add Gaussian noise to the gradient sum for differential privacy\n",
    "        noisy_gradient_sum = np.array(gaussian_mech_vec(clipped_gradient_sum, C, epsilon, delta))\n",
    "        \n",
    "        # Compute the noisy average gradient\n",
    "        noisy_avg_gradient = noisy_gradient_sum / noisy_count\n",
    "        \n",
    "        # Update the weights using the noisy average gradient\n",
    "        weights = weights - (learning_rate * noisy_avg_gradient)\n",
    "\n",
    "    return weights\n",
    "\n",
    "# Run DPSGD with specified parameters\n",
    "weights = dpsgd(1000, 1, 1e-5, 0.1, 16, 3)\n",
    "print('Final accuracy:', accuracy(weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d470559",
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilons = [0.1, 0.5, 0.4, 1.0]\n",
    "iterations_values = [100, 500, 1000, 1500, 2000, 3000]\n",
    "learning_rates = [0.01, 0.1, 0.5]\n",
    "batch_sizes = [8, 16, 32, 64]\n",
    "\n",
    "# Store results\n",
    "results = []\n",
    "\n",
    "# Iterate over parameter combinations\n",
    "for epsilon in epsilons:\n",
    "    for iterations in iterations_values:\n",
    "        for learning_rate in learning_rates:\n",
    "            for batch_size in batch_sizes:\n",
    "                # Train DPSGD model\n",
    "                weights = dpsgd(iterations, epsilon, 1e-5, learning_rate, batch_size, 3)\n",
    "                \n",
    "                # Evaluate model on test set\n",
    "                test_accuracy = accuracy(weights)\n",
    "                \n",
    "                # Store results\n",
    "                results.append({\n",
    "                    'epsilon': epsilon,\n",
    "                    'iterations': iterations,\n",
    "                    'learning_rate': learning_rate,\n",
    "                    'batch_size': batch_size,\n",
    "                    'test_accuracy': test_accuracy\n",
    "                })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce73b7c6-d7a3-447e-b98a-a23350640f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_5_accuracies = sorted(results, key=lambda x: x['test_accuracy'], reverse=True)[:5]\n",
    "print(top_5_accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53c9bd41-875b-4536-955f-741dd6f7b9da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert data to DataFrame\n",
    "df = pd.DataFrame(results)\n",
    "\n",
    "# Scatter plot of test accuracies against epsilon, colored by learning rate\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(data=df, x='epsilon', y='test_accuracy', hue='learning_rate', palette='viridis')\n",
    "plt.title('Test Accuracy vs. Epsilon')\n",
    "plt.xlabel('Epsilon')\n",
    "plt.ylabel('Test Accuracy')\n",
    "plt.legend(title='Learning Rate')\n",
    "plt.show()\n",
    "\n",
    "# Scatter plot of test accuracies against number of iterations, colored by epsilon\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(data=df, x='iterations', y='test_accuracy', hue='epsilon', palette='viridis')\n",
    "plt.title('Test Accuracy vs. Iterations')\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Test Accuracy')\n",
    "plt.legend(title='Epsilon')\n",
    "plt.show()\n",
    "\n",
    "# Scatter plot of test accuracies against batch size, colored by learning rate\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(data=df, x='batch_size', y='test_accuracy', hue='learning_rate', palette='viridis')\n",
    "plt.title('Test Accuracy vs. Batch Size')\n",
    "plt.xlabel('Batch Size')\n",
    "plt.ylabel('Test Accuracy')\n",
    "plt.legend(title='Learning Rate')\n",
    "plt.show()\n",
    "\n",
    "# Box plot of test accuracies for different epsilons\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.boxplot(data=df, x='epsilon', y='test_accuracy')\n",
    "plt.title('Test Accuracy Distribution for Different Epsilons')\n",
    "plt.xlabel('Epsilon')\n",
    "plt.ylabel('Test Accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6274e523-05d7-4947-b1ea-f28537487e53",
   "metadata": {},
   "source": [
    "#### 4\n",
    "1. Test Accuracy vs. Epsilon:\n",
    "\n",
    "- As epsilon increases (indicating lower privacy), test accuracy generally improves. This demonstrates the privacy-utility trade-off: higher privacy (lower epsilon) leads to lower accuracy, and lower privacy (higher epsilon) leads to higher accuracy.\n",
    "- But the best trade-off is 0.5 for epsilon leands to almost the same test accuracy as with epsilon 1.\n",
    " \n",
    "2. Test Accuracy vs. Iterations:\n",
    "\n",
    "- Increasing the number of iterations tends to improve test accuracy, especially for higher values of epsilon (lower privacy). This trend suggests that more iterations allow the model to better converge, even when differential privacy constraints are applied.\n",
    "- So 3000 interations leaded to the best accuracy with lower epsilon\n",
    "\n",
    "3. Test Accuracy vs. Batch Size:\n",
    "\n",
    "- Larger batch sizes generally result in better test accuracies.\n",
    "Larger batch sizes provide more stable gradient estimates, which can improve the training process. But it can have an impact on learning dynamics and generalization, but 64 still acceptable.\n",
    "- So batch size 64\n",
    "\n",
    "4. Learning Rate:\n",
    " - Lower learning rates (e.g., 0.01 or 0.1) generally yield more stable and better performance compared to higher learning rates.\n",
    " - Accroding to my plots 0.01 was the best."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bcb0f6a-f0c9-486a-94f1-ca35d1847e79",
   "metadata": {},
   "source": [
    "Based on the observed results, a recommended parameter configuration could be:\n",
    "- Epsilon: 0.5\n",
    "- Iterations: 3000\n",
    "- Learning Rate: 0.01\n",
    "- Batch Size: 64\n",
    "\n",
    "With test acc 0.805"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77a419e3-e916-4629-9801-79af7b8b2dc5",
   "metadata": {},
   "source": [
    "#### 5\n",
    "q=B/N and the costs are therefore the following: $(q*\\epsilon*\\sqrt(T), \\delta)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81a69ffc-5464-467b-a5db-e0e70cd1d116",
   "metadata": {},
   "outputs": [],
   "source": [
    "q =64/X_train.shape[0]\n",
    "cost = q * 1 * np.sqrt(1000)\n",
    "cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef0abdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot batch_sizes\n",
    "epsilon = 1\n",
    "iterations = [10, 100, 500, 1000, 2000]\n",
    "iterations_2 = [1, 2, 5, 10, 100]\n",
    "delta = 1e-5\n",
    "learning_rate = 0.1\n",
    "alpha = 20\n",
    "batch_size = [1, 4, 16, 32, 64, 128]\n",
    "C = 3\n",
    "    \n",
    "res = []\n",
    "for b in batch_size:\n",
    "    for i in iterations:\n",
    "        start = time.time()\n",
    "        acc = accuracy(dpsgd(i, epsilon, delta, learning_rate, b, C))\n",
    "        end = time.time()\n",
    "        res.append((end-start, acc, b, i, \"dspg\"))\n",
    "#oisy_gradient_descent_RDP(iterations: int, alpha: float, epsilon_bar: float) -> np.ndarray:\n",
    "    for i in iterations_2:\n",
    "        start = time.time()\n",
    "        acc_ngd = accuracy(noisy_gradient_descent_RDP(i, alpha, np.log(1/delta)/(alpha - 1)))\n",
    "        end = time.time()\n",
    "        res.append((end-start, acc_ngd, b, i, \"ngd\"))\n",
    "        \n",
    "df = pd.DataFrame(res, columns=['time', 'accuracy', 'batch_size', 'iterations', 'type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02ddd52f-cc8e-44b0-8f90-e680747c75e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a FacetGrid to separate the plots for each method\n",
    "g = sns.FacetGrid(df, col=\"type\", hue=\"iterations\", col_wrap=2, palette='colorblind', height=5)\n",
    "\n",
    "# Plot time vs. batch size for DPSG\n",
    "g.map(plt.plot, 'batch_size', 'time', marker='o', ms=8, ls='-')\n",
    "\n",
    "# Plot time vs. batch size for NGD\n",
    "g.map(plt.plot, 'batch_size', 'time', marker='s', ms=8, ls='-')\n",
    "\n",
    "# Set labels and title\n",
    "g.set_axis_labels(\"Batch Size\", \"Time (s)\")\n",
    "g.fig.suptitle('Time vs. Batch Size (Color: Iterations)', fontsize=16)\n",
    "plt.subplots_adjust(top=0.85)\n",
    "\n",
    "# Add legend\n",
    "g.add_legend(title='Iterations', fontsize=12, title_fontsize=12)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "576111a9-5436-4d6d-8713-74bac376c5af",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Create a FacetGrid to separate the plots for each method\n",
    "g = sns.FacetGrid(df, col=\"type\", hue=\"iterations\", col_wrap=2, palette='colorblind', height=5)\n",
    "\n",
    "# Plot accuracy vs. batch size for DPSG\n",
    "g.map(plt.plot, 'batch_size', 'accuracy', marker='o', ms=8, ls='-')\n",
    "\n",
    "# Plot accuracy vs. batch size for NGD\n",
    "g.map(plt.plot, 'batch_size', 'accuracy', marker='s', ms=8, ls='-')\n",
    "\n",
    "# Set labels and title\n",
    "g.set_axis_labels(\"Batch Size\", \"Accuracy\")\n",
    "g.fig.suptitle('Accuracy vs. Batch Size (Color: Iterations)', fontsize=16)\n",
    "plt.subplots_adjust(top=0.85)\n",
    "\n",
    "# Add legend\n",
    "g.add_legend(title='Iterations', fontsize=12, title_fontsize=12)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f66c77d9",
   "metadata": {},
   "source": [
    "When it comes to execution time, the batch size mainly impacts the results of DSPG because it only operates on a small batch of data rather than the entire training set. This difference in approach explains the time saved with DSPG compared to noisy gradient descent, which processes the entire training set.\n",
    "\n",
    "However, in terms of accuracy or usefulness, noisy gradient descent consistently delivers good results, whereas DSPG tends to fluctuate more, especially with smaller batch sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad37ba1a-57f8-48f2-995c-b2fb628403aa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
